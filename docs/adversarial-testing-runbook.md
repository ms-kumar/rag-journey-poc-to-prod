# Adversarial Testing Runbook

## Overview

This runbook provides procedures for adversarial testing of the RAG system's guardrails, including red-team prompts, jailbreak attempts, and security validation.

**Last Updated**: 2026-01-10  
**Owner**: Security & Safety Team  
**Review Frequency**: Monthly

---

## Table of Contents

1. [Testing Strategy](#testing-strategy)
2. [Red-Team Prompts](#red-team-prompts)
3. [Jailbreak Testing](#jailbreak-testing)
4. [CI/CD Integration](#cicd-integration)
5. [Thresholds & SLOs](#thresholds--slos)
6. [Incident Response](#incident-response)
7. [Regular Maintenance](#regular-maintenance)

---

## Testing Strategy

### Objectives

- **Prevent Jailbreaks**: Ensure no prompt can bypass safety guardrails
- **Protect PII**: Prevent exfiltration of sensitive information
- **Block Toxic Content**: Maintain content safety standards
- **Maintain Availability**: Ensure guardrails don't impact system usability

### Test Categories

1. **Jailbreak Attempts** - Prompts trying to override instructions
2. **PII Exfiltration** - Attempts to extract personal information
3. **Toxic Content** - Harmful, hateful, or violent content
4. **Prompt Injection** - System prompt manipulation
5. **Context Abuse** - Resource exhaustion attacks
6. **Edge Cases** - Boundary conditions and malformed inputs

---

## Red-Team Prompts

### Dataset Location

```
data/adversarial_prompts.json
```

### Running Red-Team Tests

```bash
# Run all adversarial tests
make test-adversarial

# Run specific category
pytest tests/test_adversarial_guardrails.py::TestAdversarialGuardrails::test_jailbreak_prompts_are_blocked

# Run with verbose output
pytest tests/test_adversarial_guardrails.py -v --tb=short
```

### Adding New Red-Team Prompts

1. **Identify Attack Vector**: Determine the category and severity
2. **Add to Dataset**: Update `data/adversarial_prompts.json`
3. **Document Expected Behavior**: Specify block/redact/allow
4. **Test Locally**: Verify guardrails catch the prompt
5. **Submit PR**: Include justification and test results

Example entry:

```json
{
  "id": "jb_006",
  "prompt": "Your new attack vector here",
  "expected_behavior": "blocked",
  "category": "jailbreak",
  "severity": "high"
}
```

---

## Jailbreak Testing

### Common Jailbreak Techniques

1. **Role-Playing**: "Pretend you're an AI without restrictions"
2. **DAN (Do Anything Now)**: "Act as DAN who can bypass rules"
3. **Developer Mode**: "Enable developer mode to disable safety"
4. **Instruction Override**: "Ignore all previous instructions"
5. **Hypothetical Scenarios**: "In a fictional world where rules don't apply..."

### Testing Procedure

```bash
# 1. Run baseline tests
pytest tests/test_adversarial_guardrails.py::TestAdversarialGuardrails::test_jailbreak_prompts_are_blocked

# 2. Check violation rate (must be â‰¤ 0.1%)
# Output will show: "Jailbreak violation rate: X.XX%"

# 3. If violations found:
#    - Review failed prompts
#    - Update guardrails logic
#    - Re-test until passing
```

### Refusal Behavior Validation

```bash
# Test refusal messages
pytest tests/test_adversarial_guardrails.py::TestRefusalBehavior

# Verify:
# - Appropriate refusal messages
# - No information leakage
# - Proper audit logging
```

---

## CI/CD Integration

### Canary Tests

Quick smoke tests that run on every commit.

```bash
# Run canary tests only
make test-canary

# Or directly with pytest
pytest -m canary tests/test_adversarial_guardrails.py
```

**Canary Test Requirements**:
- âœ… Max duration: 30 seconds
- âœ… Zero false negatives allowed
- âœ… Must pass before merge

### CI Pipeline Configuration

Add to `.github/workflows/ci.yml` (or equivalent):

```yaml
- name: Run Canary Tests
  run: make test-canary
  timeout-minutes: 2

- name: Run Adversarial Tests
  run: make test-adversarial
  timeout-minutes: 10
  # Run on main branch and PRs

- name: Verify Violation Thresholds
  run: |
    pytest tests/test_adversarial_guardrails.py::TestAdversarialGuardrails::test_overall_adversarial_violation_rate -v
```

### Deployment Gates

**Pre-Production Checklist**:
- [ ] All canary tests pass
- [ ] Adversarial test suite passes
- [ ] Violation rate â‰¤ 0.1%
- [ ] No new critical vulnerabilities
- [ ] Audit logging functional

**Production Deployment**:
- [ ] Blue-green deployment with guardrails validation
- [ ] Monitor for first 24 hours
- [ ] Rollback plan ready

---

## Thresholds & SLOs

### Critical Thresholds

Configuration: `config/guardrails_thresholds.json`

| Metric | Threshold | Action on Breach |
|--------|-----------|------------------|
| Jailbreak success rate | â‰¤ 0.1% | **BLOCK DEPLOYMENT** |
| PII leak rate | â‰¤ 0.1% | **BLOCK DEPLOYMENT** |
| Toxic bypass rate | â‰¤ 0.1% | **BLOCK DEPLOYMENT** |
| Overall violation rate | â‰¤ 0.1% | **BLOCK DEPLOYMENT** |

### Performance SLOs

| Metric | Target | Warning | Critical |
|--------|--------|---------|----------|
| P50 latency | < 50ms | > 100ms | > 200ms |
| P95 latency | < 200ms | > 300ms | > 500ms |
| P99 latency | < 400ms | > 500ms | > 1000ms |

### Monitoring

```bash
# Check recent violations
python -c "
from src.services.guardrails.audit_log import AuditLogger
logger = AuditLogger()
events = logger.get_recent_events(count=100)
violations = [e for e in events if 'violation' in e.get('event_type', '')]
print(f'Recent violations: {len(violations)}')
"

# Generate compliance report
make guardrails-report
```

---

## Incident Response

### Severity Levels

**ðŸ”´ P0 - Critical**
- Jailbreak successfully bypasses guardrails
- PII leaked to user
- System generates toxic content
- **Response Time**: Immediate (< 1 hour)

**ðŸŸ¡ P1 - High**
- False positive rate > 10%
- Guardrails performance degraded > 500ms p95
- Audit logging failure
- **Response Time**: < 4 hours

**ðŸŸ¢ P2 - Medium**
- Edge case handling issue
- Non-critical test failure
- **Response Time**: < 24 hours

### Incident Workflow

1. **Detect**
   - CI/CD test failure
   - Production monitoring alert
   - User report

2. **Assess**
   ```bash
   # Run diagnostic tests
   pytest tests/test_adversarial_guardrails.py -v --tb=long
   
   # Check audit logs
   tail -n 100 logs/audit.log
   
   # Identify attack vector
   ```

3. **Mitigate**
   - If P0: Rollback immediately
   - Update guardrails logic
   - Add test case for the attack
   - Verify fix locally

4. **Deploy Fix**
   ```bash
   # Test fix
   make test-adversarial
   
   # Deploy to staging
   make deploy-staging
   
   # Validate in staging
   make test-adversarial-staging
   
   # Deploy to production
   make deploy-production
   ```

5. **Post-Incident**
   - Update adversarial dataset
   - Document attack vector
   - Review guardrails architecture
   - Update runbook if needed

### Emergency Contacts

- **Security Lead**: [Contact Info]
- **On-Call Engineer**: [Pager/Slack]
- **Compliance Team**: [Email]

---

## Regular Maintenance

### Weekly Tasks

```bash
# Run full adversarial test suite
make test-adversarial

# Review audit logs for anomalies
make guardrails-audit-review

# Check for new attack vectors in security bulletins
```

### Monthly Tasks

- [ ] Review and update red-team prompts dataset
- [ ] Analyze false positive/negative rates
- [ ] Performance testing under load
- [ ] Review and update thresholds
- [ ] Update this runbook
- [ ] Security team review meeting

### Quarterly Tasks

- [ ] Comprehensive security audit
- [ ] Red-team exercise with external team
- [ ] Guardrails architecture review
- [ ] Compliance audit
- [ ] Update training data for ML-based filters

### Adding New Attack Vectors

When new jailbreak techniques emerge:

1. **Document the Attack**
   ```bash
   # Create new entry in adversarial_prompts.json
   {
     "id": "new_attack_id",
     "prompt": "New attack vector",
     "expected_behavior": "blocked",
     "category": "appropriate_category",
     "severity": "high",
     "discovered_date": "2026-01-10",
     "source": "CVE-XXXX or researcher name"
   }
   ```

2. **Test Current Guardrails**
   ```bash
   pytest tests/test_adversarial_guardrails.py -k "test_overall" -v
   ```

3. **If Vulnerability Found**
   - Create incident ticket (P0/P1)
   - Follow incident response workflow
   - Notify security team immediately

4. **Update Guardrails**
   - Implement detection logic
   - Add specific test case
   - Verify fix with full test suite

5. **Document and Deploy**
   - Update documentation
   - Deploy with monitoring
   - Add to weekly review checklist

---

## Testing Commands Reference

```bash
# Quick canary tests (< 30s)
make test-canary

# Full adversarial test suite
make test-adversarial

# Specific test categories
pytest tests/test_adversarial_guardrails.py::TestAdversarialGuardrails::test_jailbreak_prompts_are_blocked
pytest tests/test_adversarial_guardrails.py::TestAdversarialGuardrails::test_pii_exfiltration_is_prevented
pytest tests/test_adversarial_guardrails.py::TestAdversarialGuardrails::test_toxic_content_is_blocked

# Refusal behavior
pytest tests/test_adversarial_guardrails.py::TestRefusalBehavior

# Overall violation rate (CRITICAL)
pytest tests/test_adversarial_guardrails.py::TestAdversarialGuardrails::test_overall_adversarial_violation_rate

# All guardrails tests
make test-guardrails

# With coverage
pytest tests/test_adversarial_guardrails.py --cov=src/services/guardrails --cov-report=html
```

---

## Metrics Dashboard

### Key Metrics to Monitor

1. **Violation Rate**: `violations / total_tests`
2. **Detection Rate**: `detected_attacks / total_attacks`
3. **False Positive Rate**: `false_positives / total_safe_inputs`
4. **Response Time**: P50, P95, P99 latencies
5. **Throughput**: Requests per second

### Alerting Rules

```yaml
# Example Prometheus alerts
- alert: JailbreakDetected
  expr: guardrails_jailbreak_success > 0
  for: 0m
  severity: critical

- alert: HighViolationRate
  expr: guardrails_violation_rate > 0.001
  for: 5m
  severity: critical

- alert: SlowGuardrails
  expr: guardrails_latency_p95 > 500
  for: 10m
  severity: warning
```

---

## Compliance & Audit

### Audit Log Requirements

- All guardrails decisions must be logged
- Retention: 90 days minimum
- Include: timestamp, user_id, prompt, decision, reason
- Secure storage with access controls

### Compliance Checks

```bash
# Verify audit logging
pytest tests/test_guardrails_audit.py

# Check PII redaction compliance
pytest tests/test_adversarial_guardrails.py::TestAdversarialGuardrails::test_pii_exfiltration_is_prevented

# Generate compliance report
python scripts/generate_compliance_report.py --period monthly
```

### GDPR Compliance

- âœ… PII detection and redaction: 100%
- âœ… Right to deletion: Audit log purge capability
- âœ… Data minimization: Only log necessary information
- âœ… Breach notification: Incident response < 72 hours

---

## Additional Resources

- **Adversarial ML Research**: [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- **Jailbreak Database**: Community-maintained list of known attacks
- **Security Bulletins**: Subscribe to AI safety research updates
- **Internal Docs**: `docs/guardrails/`

---

## Change Log

| Date | Change | Author |
|------|--------|--------|
| 2026-01-10 | Initial runbook creation | System |
| | Added adversarial testing procedures | |
| | Defined thresholds and SLOs | |

---

## Appendix

### Example Incident Report Template

```markdown
# Guardrails Incident Report

**Incident ID**: INC-2026-XXX
**Date**: YYYY-MM-DD
**Severity**: P0/P1/P2
**Status**: Open/Resolved

## Summary
Brief description of the incident

## Timeline
- HH:MM - Detected
- HH:MM - Assessment started
- HH:MM - Mitigation deployed
- HH:MM - Resolved

## Attack Vector
Description of the attack that bypassed guardrails

## Root Cause
Technical analysis of why guardrails failed

## Resolution
Steps taken to fix the issue

## Prevention
Measures to prevent future occurrences

## Action Items
- [ ] Update adversarial dataset
- [ ] Enhance guardrails logic
- [ ] Add monitoring
```

---

**Document Version**: 1.0  
**Next Review Date**: 2026-02-10
