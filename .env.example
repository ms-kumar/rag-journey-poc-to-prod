# ============================================
# Advanced RAG - Environment Configuration
# ============================================
# Copy this file to .env and update values as needed.
# For production, use actual secrets and secure key management.

# ============================================
# Application Configuration
# ============================================
APP_NAME=Advanced RAG API
APP_VERSION=1.0.0
ENVIRONMENT=development  # development, staging, production
DEBUG=false
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

 # ============================================
# Server Configuration
# ============================================
HOST=0.0.0.0
PORT=8000

# ============================================
# Ingestion Configuration
# ============================================
INGESTION_DIR=./data
INGEST_FORMATS=*.txt,*.md,*.html,*.pdf  # Comma-separated patterns

# ============================================
# Chunking Configuration
# ============================================
CHUNK_SIZE=200  # Characters per chunk
CHUNK_OVERLAP=50  # Characters overlap between chunks
CHUNK_METHOD=fixed  # fixed, heading_aware

# ============================================
# Embedding Configuration
# ============================================
EMBED_PROVIDER=hash  # hash, e5, bge, huggingface, openai, cohere
EMBED_MODEL=simple-hash  # Model name depends on provider
EMBED_DIM=64  # Embedding dimension (only for hash provider)
EMBED_DEVICE=cpu  # cpu, cuda, mps (for local models)
EMBED_BATCH_SIZE=32
EMBED_NORMALIZE=true
# EMBED_API_KEY=  # Required for OpenAI/Cohere providers

# --- Embedding Cache Settings ---
EMBED_CACHE_ENABLED=true  # Enable embedding caching
EMBED_CACHE_MAX_SIZE=10000  # Max embeddings to cache in memory
EMBED_CACHE_DIR=.cache/embeddings  # Cache directory (None to disable disk persistence)

# --- Provider-Specific Examples ---
# E5 Embeddings (local):
# EMBED_PROVIDER=e5
# EMBED_MODEL=intfloat/e5-small-v2  # or e5-small, e5-base, e5-large
# EMBED_DEVICE=cuda

# BGE Embeddings (local):
# EMBED_PROVIDER=bge
# EMBED_MODEL=BAAI/bge-small-en-v1.5  # or bge-small, bge-base, bge-large
# EMBED_DEVICE=cuda

# OpenAI Embeddings (API):
# EMBED_PROVIDER=openai
# EMBED_MODEL=text-embedding-3-small  # or text-embedding-3-large, text-embedding-ada-002
# EMBED_API_KEY=sk-proj-...

# Cohere Embeddings (API):
# EMBED_PROVIDER=cohere
# EMBED_MODEL=embed-english-v3.0  # or embed-english-light-v3.0, embed-multilingual-v3.0
# EMBED_API_KEY=...

# ============================================
# Vector Store (Qdrant) Configuration
# ============================================
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=  # Optional for Qdrant Cloud
QDRANT_COLLECTION_NAME=naive_collection
QDRANT_PREFER_GRPC=true

# ============================================
# Generation (LLM) Configuration
# ============================================
GENERATOR_MODEL=gpt2  # HuggingFace model or OpenAI model name
GENERATOR_DEVICE=cpu  # cpu, cuda, mps (for local models)
GENERATOR_MAX_LENGTH=128
GENERATOR_TEMPERATURE=1.0

# For OpenAI LLMs:
# GENERATOR_MODEL=gpt-4o-mini
# GENERATOR_API_KEY=sk-...
# GENERATOR_MAX_TOKENS=500

# ============================================
# RAG Pipeline Configuration
# ============================================
RAG_TOP_K=5  # Number of chunks to retrieve
RAG_MAX_CONTEXT_DOCS=3  # Max documents to use in context
RETRIEVAL_MIN_SCORE=0.0  # Minimum similarity score (0.0 to 1.0)

# ============================================
# Development Settings
# ============================================
DEV_RELOAD=true  # Auto-reload on code changes (uvicorn --reload)
# DEV_DEBUG=true  # Enable additional debug logging