# ============================================
# Advanced RAG - Environment Configuration
# ============================================
# Copy this file to .env and update values as needed.
# For production, use actual secrets and secure key management.
# 
# NOTE: All settings use double underscore (__) as delimiter
# Format: <PREFIX>__<SETTING>=<VALUE>

# ============================================
# Application Configuration
# ============================================
APP__NAME=Advanced RAG API
APP__VERSION=1.0.0
APP__DEBUG=false
APP__LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# ============================================
# Server Configuration
# ============================================
SERVER__HOST=0.0.0.0
SERVER__PORT=8000

# ============================================
# Ingestion Configuration
# ============================================
INGESTION__DIR=./data

# ============================================
# Chunking Configuration
# ============================================
CHUNKING__CHUNK_SIZE=200  # Characters per chunk
CHUNKING__CHUNK_OVERLAP=50  # Characters overlap between chunks
CHUNKING__STRATEGY=heading_aware  # fixed, heading_aware

# ============================================
# Embedding Configuration
# ============================================
EMBED__PROVIDER=hash  # hash, e5, bge, huggingface, openai, cohere
EMBED__MODEL=simple-hash  # Model name depends on provider
EMBED__DIM=64  # Embedding dimension (only for hash provider)
EMBED__DEVICE=cpu  # cpu, cuda, mps (for local models)
EMBED__BATCH_SIZE=32
EMBED__NORMALIZE=true
# EMBED__API_KEY=  # Required for OpenAI/Cohere providers

# --- Embedding Cache Settings ---
EMBED__CACHE_ENABLED=true  # Enable embedding caching
EMBED__CACHE_MAX_SIZE=10000  # Max embeddings to cache in memory
EMBED__CACHE_DIR=.cache/embeddings  # Cache directory (None to disable disk persistence)

# --- Provider-Specific Examples ---
# E5 Embeddings (local):
# EMBED__PROVIDER=e5
# EMBED__MODEL=intfloat/e5-small-v2  # or e5-small, e5-base, e5-large
# EMBED__DEVICE=cuda

# BGE Embeddings (local):
# EMBED__PROVIDER=bge
# EMBED__MODEL=BAAI/bge-small-en-v1.5  # or bge-small, bge-base, bge-large
# EMBED__DEVICE=cuda

# OpenAI Embeddings (API):
# EMBED__PROVIDER=openai
# EMBED__MODEL=text-embedding-3-small  # or text-embedding-3-large, text-embedding-ada-002
# EMBED__API_KEY=sk-proj-...

# Cohere Embeddings (API):
# EMBED__PROVIDER=cohere
# EMBED__MODEL=embed-english-v3.0  # or embed-english-light-v3.0, embed-multilingual-v3.0
# EMBED__API_KEY=...

# ============================================
# Vector Store (Qdrant) Configuration
# ============================================
QDRANT__URL=http://localhost:6333
QDRANT__API_KEY=  # Optional for Qdrant Cloud
QDRANT__COLLECTION_NAME=naive_collection
QDRANT__PREFER_GRPC=true
QDRANT__ENABLE_BM25=false  # Enable BM25 text indexing for hybrid search

# ============================================
# Generation (LLM) Configuration
# ============================================
GENERATOR__MODEL=gpt2  # HuggingFace model or OpenAI model name
GENERATOR__DEVICE=  # -1 for CPU, 0+ for GPU (HuggingFace only)
GENERATOR__MAX_LENGTH=128
GENERATOR__TEMPERATURE=1.0

# For OpenAI LLMs:
# GENERATOR__MODEL=gpt-4o-mini
# GENERATOR__API_KEY=sk-...
# GENERATOR__MAX_TOKENS=500

# ============================================
# RAG Pipeline Configuration
# ============================================
RAG__TOP_K=5  # Number of chunks to retrieve
RAG__MAX_CONTEXT_DOCS=3  # Max documents to use in context

# ============================================
# Backward Compatibility (Legacy Format)
# ============================================
# The following legacy format is still supported but deprecated:
# APP_NAME, HOST, PORT, CHUNK_SIZE, EMBED_PROVIDER, etc.
# Please migrate to the new format with __ delimiter.